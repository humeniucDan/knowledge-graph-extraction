{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T20:35:06.846355Z",
     "start_time": "2025-12-05T20:35:05.156215Z"
    }
   },
   "source": [
    "import os\n",
    "import spacy\n",
    "from datasets import load_dataset, ClassLabel, Sequence\n",
    "\n",
    "from consts import DATA_PATH"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/Work/utcn/an4/sem1/pso/proj/knowledge-graph-extraction/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T20:35:06.857965Z",
     "start_time": "2025-12-05T20:35:06.856008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Setup Paths\n",
    "RAW_CACHE_PATH = os.path.join(DATA_PATH, \"conll2003_raw_cache\")\n",
    "PROCESSED_DATA_PATH = os.path.join(DATA_PATH, \"conll2003_augmented_pos\")\n",
    "SPACY_MODEL_NAME = \"en_core_web_lg\""
   ],
   "id": "3620c6f60e9a7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T20:35:07.243690Z",
     "start_time": "2025-12-05T20:35:07.086632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Load spaCy for POS tagging\n",
    "nlp = spacy.load(SPACY_MODEL_NAME, disable=[\"parser\", \"ner\", \"lemmatizer\"])\n",
    "\n",
    "# 3. Load and Cache the Raw CoNLL-2003 Dataset\n",
    "print(f\"Downloading/Loading CoNLL-2003 to {RAW_CACHE_PATH}...\")\n",
    "dataset = load_dataset(\"conll2003\", cache_dir=RAW_CACHE_PATH)"
   ],
   "id": "4d87792606009509",
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOSError\u001B[39m                                   Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# 2. Load spaCy for POS tagging\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m nlp = \u001B[43mspacy\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43men_core_web_sm\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdisable\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mparser\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mner\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# 3. Load and Cache the Raw CoNLL-2003 Dataset\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mDownloading/Loading CoNLL-2003 to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mRAW_CACHE_PATH\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m...\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work/utcn/an4/sem1/pso/proj/knowledge-graph-extraction/.venv/lib/python3.13/site-packages/spacy/__init__.py:52\u001B[39m, in \u001B[36mload\u001B[39m\u001B[34m(name, vocab, disable, enable, exclude, config)\u001B[39m\n\u001B[32m     28\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mload\u001B[39m(\n\u001B[32m     29\u001B[39m     name: Union[\u001B[38;5;28mstr\u001B[39m, Path],\n\u001B[32m     30\u001B[39m     *,\n\u001B[32m   (...)\u001B[39m\u001B[32m     35\u001B[39m     config: Union[Dict[\u001B[38;5;28mstr\u001B[39m, Any], Config] = util.SimpleFrozenDict(),\n\u001B[32m     36\u001B[39m ) -> Language:\n\u001B[32m     37\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001B[39;00m\n\u001B[32m     38\u001B[39m \n\u001B[32m     39\u001B[39m \u001B[33;03m    name (str): Package name or model path.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     50\u001B[39m \u001B[33;03m    RETURNS (Language): The loaded nlp object.\u001B[39;00m\n\u001B[32m     51\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mutil\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     53\u001B[39m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     54\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvocab\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     55\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdisable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     56\u001B[39m \u001B[43m        \u001B[49m\u001B[43menable\u001B[49m\u001B[43m=\u001B[49m\u001B[43menable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     57\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexclude\u001B[49m\u001B[43m=\u001B[49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     58\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     59\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work/utcn/an4/sem1/pso/proj/knowledge-graph-extraction/.venv/lib/python3.13/site-packages/spacy/util.py:531\u001B[39m, in \u001B[36mload_model\u001B[39m\u001B[34m(name, vocab, disable, enable, exclude, config)\u001B[39m\n\u001B[32m    529\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m OLD_MODEL_SHORTCUTS:\n\u001B[32m    530\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors.E941.format(name=name, full=OLD_MODEL_SHORTCUTS[name]))  \u001B[38;5;66;03m# type: ignore[index]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m531\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors.E050.format(name=name))\n",
      "\u001B[31mOSError\u001B[39m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 4. Prepare the New Label Set\n",
    "original_features = dataset[\"train\"].features[\"ner_tags\"].feature\n",
    "original_label_names = original_features.names\n",
    "\n",
    "# Define new tags to add\n",
    "new_tags = [\"B-NOUN\", \"I-NOUN\", \"B-PRON\", \"I-PRON\"]\n",
    "final_label_names = original_label_names + new_tags\n",
    "\n",
    "# Create mappings\n",
    "label2id = {label: i for i, label in enumerate(final_label_names)}\n",
    "id2label = {i: label for i, label in enumerate(final_label_names)}\n",
    "\n",
    "print(f\"Original tags: {original_label_names}\")\n",
    "print(f\"New tags added: {new_tags}\")\n",
    "\n",
    "# 5. Define the Augmentation Function\n",
    "def augment_batch(batch):\n",
    "    new_batch_ner_tags = []\n",
    "\n",
    "    # Process texts in a batch for speed\n",
    "    # We use spacy.tokens.Doc to create docs directly from pre-tokenized lists\n",
    "    # to ensure alignment with CoNLL tokens.\n",
    "    docs = [spacy.tokens.Doc(nlp.vocab, words=tokens) for tokens in batch[\"tokens\"]]\n",
    "\n",
    "    # Run the tagger on the batch of docs\n",
    "    for doc in nlp.pipe(docs):\n",
    "        pass # The doc is modified in place by the tagger pipe\n",
    "\n",
    "    # Iterate over the batch\n",
    "    for i, doc in enumerate(docs):\n",
    "        original_tags = batch[\"ner_tags\"][i]\n",
    "        augmented_tags = []\n",
    "\n",
    "        for token, original_id in zip(doc, original_tags):\n",
    "            original_label = original_label_names[original_id]\n",
    "\n",
    "            # RULE: Keep existing NER tags (Persons, Orgs, Locs)\n",
    "            if original_label != \"O\":\n",
    "                augmented_tags.append(label2id[original_label])\n",
    "\n",
    "            # RULE: If 'O', check for Noun/Pronoun via spaCy\n",
    "            else:\n",
    "                if token.pos_ == \"NOUN\":\n",
    "                    augmented_tags.append(label2id[\"B-NOUN\"])\n",
    "                elif token.pos_ == \"PRON\":\n",
    "                    augmented_tags.append(label2id[\"B-PRON\"])\n",
    "                else:\n",
    "                    augmented_tags.append(label2id[\"O\"])\n",
    "\n",
    "        new_batch_ner_tags.append(augmented_tags)\n",
    "\n",
    "    return {\"ner_tags\": new_batch_ner_tags}\n"
   ],
   "id": "b79e7c2a79d88556",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 6. Apply Augmentation\n",
    "print(\"Augmenting dataset (this might take a minute)...\")\n",
    "augmented_dataset = dataset.map(augment_batch, batched=True)\n"
   ],
   "id": "9cc627b7546f2114",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 7. Cast Features to the New Label Set\n",
    "# This is crucial so the dataset knows \"9\" maps to \"B-NOUN\"\n",
    "new_features = augmented_dataset[\"train\"].features.copy()\n",
    "new_features[\"ner_tags\"] = Sequence(ClassLabel(names=final_label_names))\n",
    "\n",
    "print(\"Casting features to new label set...\")\n",
    "augmented_dataset = augmented_dataset.cast(new_features)\n"
   ],
   "id": "fedb3d4af214e505",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 8. Save the Final Processed Dataset\n",
    "print(f\"Saving augmented dataset to {PROCESSED_DATA_PATH}...\")\n",
    "augmented_dataset.save_to_disk(PROCESSED_DATA_PATH)\n",
    "\n",
    "print(\"Done! Dataset is ready for training.\")"
   ],
   "id": "9265a1ea54d4a9f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
