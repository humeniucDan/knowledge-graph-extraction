{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T00:05:07.598006Z",
     "start_time": "2025-12-06T00:05:07.596075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from datasets import load_dataset, ClassLabel, Sequence\n",
    "from consts import DATA_PATH\n",
    "import spacy"
   ],
   "id": "8809640e4b0d4e4a",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T00:05:07.621184Z",
     "start_time": "2025-12-06T00:05:07.619429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# config\n",
    "MODEL_NAME = \"en_core_web_trf\"\n",
    "SAVE_PATH = os.path.join(DATA_PATH, \"wikiann+spacy_pos\")"
   ],
   "id": "3b0197b926763535",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T00:05:11.120807Z",
     "start_time": "2025-12-06T00:05:07.668440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load\n",
    "nlp = spacy.load(MODEL_NAME, disable=[\"parser\", \"ner\", \"lemmatizer\"])\n",
    "dataset = load_dataset('wikiann', 'en', cache_dir=os.path.join(DATA_PATH, \"raw_cache\"))"
   ],
   "id": "93a1b494bcfeb8ba",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T00:05:11.175983Z",
     "start_time": "2025-12-06T00:05:11.172393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "original_features = dataset[\"train\"].features[\"ner_tags\"].feature\n",
    "original_names = original_features.names\n",
    "new_names = original_names + [\"B-NOUN\", \"I-NOUN\", \"B-PRON\", \"I-PRON\"]\n",
    "label2id = {label: i for i, label in enumerate(new_names)}"
   ],
   "id": "33351e5f52516d84",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T00:05:11.236151Z",
     "start_time": "2025-12-06T00:05:11.232933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def doc_generator(tokens_list):\n",
    "    for tokens in tokens_list:\n",
    "        yield Doc(nlp.vocab, words=tokens)"
   ],
   "id": "a49f099a01434a43",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T00:05:11.307253Z",
     "start_time": "2025-12-06T00:05:11.294135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define the Logic\n",
    "def augment_batch(batch):\n",
    "    new_batch_tags = []\n",
    "\n",
    "    # Use the generator here instead of a list comprehension\n",
    "    doc_gen = doc_generator(batch[\"tokens\"])\n",
    "\n",
    "    # Process with nlp.pipe\n",
    "    # batch_size=32 is safer for Transformers to avoid index mismatch errors\n",
    "    processed_docs = list(nlp.pipe(doc_gen, batch_size=32))\n",
    "\n",
    "    for i, doc in enumerate(processed_docs):\n",
    "        original_ids = batch[\"ner_tags\"][i]\n",
    "        row_tags = []\n",
    "\n",
    "        # doc is now fully processed with POS tags\n",
    "        for token, original_id in zip(doc, original_ids):\n",
    "            original_label = original_names[original_id]\n",
    "\n",
    "            # 1. Keep Entities\n",
    "            if original_label != \"O\":\n",
    "                row_tags.append(label2id[original_label])\n",
    "            # 2. Augment Nouns/Pronouns\n",
    "            else:\n",
    "                if token.pos_ == \"NOUN\":\n",
    "                    row_tags.append(label2id[\"B-NOUN\"])\n",
    "                elif token.pos_ == \"PRON\":\n",
    "                    row_tags.append(label2id[\"B-PRON\"])\n",
    "                else:\n",
    "                    row_tags.append(label2id[\"O\"])\n",
    "\n",
    "        new_batch_tags.append(row_tags)\n",
    "\n",
    "    # Return NEW column\n",
    "    return {\"augmented_tags\": new_batch_tags}"
   ],
   "id": "4ceede2235057bcd",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T00:05:11.358126Z",
     "start_time": "2025-12-06T00:05:11.354642Z"
    }
   },
   "cell_type": "code",
   "source": "spacy.prefer_gpu()",
   "id": "17220eef2bc36e5d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T00:06:31.531510Z",
     "start_time": "2025-12-06T00:05:11.406757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# run\n",
    "from spacy.tokens import Doc\n",
    "augmented_dataset = dataset.map(augment_batch, batched=True, batch_size=50)\n",
    "\n",
    "augmented_dataset = augmented_dataset.remove_columns(\"ner_tags\")\n",
    "augmented_dataset = augmented_dataset.rename_column(\"augmented_tags\", \"ner_tags\")"
   ],
   "id": "5789dd55a0a8d9df",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]/home/dan/Work/utcn/an4/sem1/pso/proj/knowledge-graph-extraction/.venv1/lib/python3.11/site-packages/thinc/util.py:395: VisibleDeprecationWarning: This function is deprecated and will be removed in a future release. Use the cupy.from_dlpack() array constructor instead.\n",
      "  dlpack_tensor = xp_tensor.toDlpack()  # type: ignore\n",
      "Map: 100%|██████████| 10000/10000 [00:20<00:00, 497.99 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:18<00:00, 529.45 examples/s]\n",
      "Map: 100%|██████████| 20000/20000 [00:37<00:00, 531.79 examples/s]\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T00:06:31.668819Z",
     "start_time": "2025-12-06T00:06:31.606808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save\n",
    "print(f\"Saving to {SAVE_PATH}...\")\n",
    "# Update the feature definition so the dataset knows about the new tags\n",
    "new_features = augmented_dataset[\"train\"].features.copy()\n",
    "new_features[\"ner_tags\"] = Sequence(ClassLabel(names=new_names))\n",
    "augmented_dataset = augmented_dataset.cast(new_features)\n",
    "\n",
    "augmented_dataset.save_to_disk(SAVE_PATH)\n",
    "print(\"Success! Dataset created.\")"
   ],
   "id": "96391b7bfdb61e4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to /home/dan/Work/utcn/an4/sem1/pso/proj/knowledge-graph-extraction/train/data/conll2003_augmented_lg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 10000/10000 [00:00<00:00, 1328362.31 examples/s]\n",
      "Casting the dataset: 100%|██████████| 10000/10000 [00:00<00:00, 1483606.52 examples/s]\n",
      "Casting the dataset: 100%|██████████| 20000/20000 [00:00<00:00, 2116464.74 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 1875387.44 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 1582159.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 20000/20000 [00:00<00:00, 2175638.15 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Dataset created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 33
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
