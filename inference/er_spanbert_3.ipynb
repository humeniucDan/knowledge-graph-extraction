{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T22:29:02.085124Z",
     "start_time": "2025-12-04T22:28:59.737433Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.nn.functional import cosine_similarity"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/Work/utcn/an4/sem1/pso/proj/knowledge-graph-extraction/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T22:29:03.657917Z",
     "start_time": "2025-12-04T22:29:03.653799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "project_cache_dir = \"./hf_cache\"\n",
    "os.makedirs(project_cache_dir, exist_ok=True)\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = project_cache_dir\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = project_cache_dir"
   ],
   "id": "eb81edf186480fa1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T22:29:47.294740Z",
     "start_time": "2025-12-04T22:29:45.867216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load SpanBERT directly from Hugging Face\n",
    "# SpanBERT is architecturally just BERT, so AutoModel loads it perfectly.\n",
    "model_name = \"SpanBERT/spanbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=project_cache_dir)\n",
    "model = AutoModel.from_pretrained(model_name, cache_dir=project_cache_dir)\n"
   ],
   "id": "d4e46223f4f935f9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-base-cased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T22:29:48.254184Z",
     "start_time": "2025-12-04T22:29:48.043822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Move to GPU (8GB VRAM is plenty for this)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ],
   "id": "f91bf5a27bdb2f8f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T22:29:51.322330Z",
     "start_time": "2025-12-04T22:29:51.319305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Your Input\n",
    "text = \"Lionel Messi scored a brilliant goal. The Argentine forward celebrated with his teammates. He is considered the best.\"\n",
    "# 3. Prepare Inputs\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n"
   ],
   "id": "40141688cac20242",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T22:29:52.303762Z",
     "start_time": "2025-12-04T22:29:52.230353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Run the Model (Forward Pass)\n",
    "# This runs the self-attention layers. The output 'last_hidden_state'\n",
    "# contains the vectors that \"know\" the context.\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Shape: [Batch_Size, Sequence_Length, Hidden_Size (768)]\n",
    "embeddings = outputs.last_hidden_state[0]\n"
   ],
   "id": "b102cc0439394be6",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T22:30:00.447802Z",
     "start_time": "2025-12-04T22:30:00.441289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- CRITICAL STEP: Mapping Entities to Token Indices ---\n",
    "# In a real pipeline, your BIO tagger gives you these offsets.\n",
    "# Let's find where the tokens are manually for this test.\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "# Helper to find token range for a substring\n",
    "def find_span(substring, all_tokens):\n",
    "    # This is a naive finder for demonstration\n",
    "    sub_tokens = tokenizer.tokenize(substring)\n",
    "    length = len(sub_tokens)\n",
    "    for i in range(len(all_tokens) - length):\n",
    "        if all_tokens[i:i+length] == sub_tokens:\n",
    "            return i, i + length # Start, End (exclusive)\n",
    "    return None, None\n",
    "\n",
    "# Find positions\n",
    "messi_start, messi_end = find_span(\"Lionel Messi\", tokens)\n",
    "he_start, he_end = find_span(\"He\", tokens)\n",
    "goal_start, goal_end = find_span(\"goal\", tokens)\n",
    "\n",
    "print(f\"Indices -> Messi: {messi_start}-{messi_end}, He: {he_start}-{he_end}, Goal: {goal_start}-{goal_end}\")\n"
   ],
   "id": "558f9a38bd142a89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices -> Messi: 1-5, He: 20-21, Goal: 8-9\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T22:30:02.970244Z",
     "start_time": "2025-12-04T22:30:02.958250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5. Extract Span Representations (Average Pooling)\n",
    "# We average the embeddings of the tokens in the span to get one vector per entity.\n",
    "messi_vec = torch.mean(embeddings[messi_start:messi_end], dim=0).unsqueeze(0)\n",
    "he_vec    = torch.mean(embeddings[he_start:he_end], dim=0).unsqueeze(0)\n",
    "goal_vec  = torch.mean(embeddings[goal_start:goal_end], dim=0).unsqueeze(0)\n"
   ],
   "id": "9000af17910e4603",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T22:30:04.451997Z",
     "start_time": "2025-12-04T22:30:04.417342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 6. Calculate Similarity\n",
    "# High similarity means the Self-Attention mechanism contextualized them similarly.\n",
    "score_messi_he = cosine_similarity(messi_vec, he_vec).item()\n",
    "score_messi_goal = cosine_similarity(messi_vec, goal_vec).item()\n",
    "\n",
    "print(\"\\n--- Results ---\")\n",
    "print(f\"Similarity ('Lionel Messi' vs 'He'):   {score_messi_he:.4f}\")\n",
    "print(f\"Similarity ('Lionel Messi' vs 'goal'): {score_messi_goal:.4f}\")\n",
    "\n",
    "if score_messi_he > score_messi_goal:\n",
    "    print(\">> 'He' refers to 'Lionel Messi'\")\n",
    "else:\n",
    "    print(\">> 'He' refers to 'goal'\")"
   ],
   "id": "89b45b491e1321e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results ---\n",
      "Similarity ('Lionel Messi' vs 'He'):   0.7052\n",
      "Similarity ('Lionel Messi' vs 'goal'): 0.5806\n",
      ">> 'He' refers to 'Lionel Messi'\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
