{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from datasets import load_dataset\n",
    "\n",
    "import consts\n",
    "\n",
    "# 1. Setup spaCy for POS tagging\n",
    "# We disable 'parser' and 'ner' to make it faster; we only need the tagger\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# 2. Load standard NER dataset (CoNLL-2003)\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "\n",
    "# 3. Define the Label Mappings\n",
    "# CoNLL original: {0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', ...}\n",
    "# We need to map integers to strings to manipulate them easily\n",
    "original_feature = dataset['train'].features['ner_tags'].feature\n",
    "label_names = original_feature.names\n",
    "\n",
    "# Add our new custom labels\n",
    "new_label_names = label_names + [\"B-NOUN\", \"I-NOUN\", \"B-PRON\", \"I-PRON\"]\n",
    "label2id = {label: i for i, label in enumerate(new_label_names)}\n",
    "id2label = {i: label for i, label in enumerate(new_label_names)}\n",
    "\n",
    "def augment_data(example):\n",
    "    tokens = example['tokens']\n",
    "    ner_ids = example['ner_tags']\n",
    "    \n",
    "    # Create a spaCy Doc from the pre-tokenized list (preserves alignment)\n",
    "    doc = nacy_doc = spacy.tokens.Doc(nlp.vocab, words=tokens)\n",
    "    # Run the tagger pipeline manually on this doc\n",
    "    nlp.tagger(doc)\n",
    "    \n",
    "    new_tags = []\n",
    "    \n",
    "    for token, ner_id in zip(doc, ner_ids):\n",
    "        original_tag = label_names[ner_id]\n",
    "        \n",
    "        # LOGIC:\n",
    "        # 1. If it already has an NER tag (e.g., B-PER), KEEP IT.\n",
    "        # 2. If it is 'O', check POS tags.\n",
    "        if original_tag != \"O\":\n",
    "            new_tags.append(label2id[original_tag])\n",
    "        \n",
    "        else:\n",
    "            # Check spaCy POS tags\n",
    "            # standard spaCy POS: NOUN, PROPN, PRON\n",
    "            if token.pos_ in [\"NOUN\"]:\n",
    "                new_tags.append(label2id[\"B-NOUN\"]) # Simplified: treating all as Beginning\n",
    "            elif token.pos_ == \"PRON\":\n",
    "                new_tags.append(label2id[\"B-PRON\"])\n",
    "            else:\n",
    "                new_tags.append(label2id[\"O\"])\n",
    "                \n",
    "    return {'ner_tags': new_tags}\n",
    "\n",
    "# 4. Apply augmentation to the dataset\n",
    "print(\"Augmenting dataset with POS tags...\")\n",
    "augmented_dataset = dataset.map(augment_data)\n",
    "\n",
    "# Verify one example\n",
    "print(f\"Tokens: {augmented_dataset['train'][0]['tokens']}\")\n",
    "print(f\"New IDs: {augmented_dataset['train'][0]['ner_tags']}\")\n",
    "print(f\"Readable: {[id2label[i] for i in augmented_dataset['train'][0]['ner_tags']]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
